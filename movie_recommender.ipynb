{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### File download\n",
    "First of all, let's get the data. GroupLens Research has collected movie rating data sets and made it available from [MoiveLens web site](https://grouplens.org/datasets/movielens/). There are two datasets:\n",
    "- Small: 100,000 ratings and 1,300 tag applications applied to 9,000 movies by 700 users. Last updated 10/2016. \n",
    "- Full: 24,000,000 ratings and 670,000 tag applications applied to 40,000 movies by 260,000 users. Includes tag genome data with 12 million relevance scores across 1,100 tags. Last updated 10/2016.\n",
    "\n",
    "For now, we just use the small dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "smallset_url = 'http://files.grouplens.org/datasets/movielens/ml-latest-small.zip'\n",
    "fullset_url = 'http://files.grouplens.org/datasets/movielens/ml-latest.zip'\n",
    "\n",
    "import os\n",
    "\n",
    "smallset_path = os.path.join('data', 'ml-latest-small.zip')\n",
    "fullset_path = os.path.join('data', 'ml-latest.zip')\n",
    "\n",
    "import urllib\n",
    "smallset = urllib.urlretrieve(smallset_url, smallset_path)\n",
    "fullset = urllib.urlretrieve(fullset_url, fullset_path)\n",
    "\n",
    "import zipfile\n",
    "with zipfile.ZipFile(smallset_path, \"r\") as z:\n",
    "    z.extractall('data')\n",
    "with zipfile.ZipFile(fullset_path, \"r\") as z:\n",
    "    z.extractall('data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Analysing datasets\n",
    "Now we take a closer look at the dataset.\n",
    "\n",
    "Each line in the ratings dataset (*ratings.csv*) is formatted as:\n",
    "\n",
    "*userId,movieId,rating,timestamp*\n",
    "\n",
    "Each line in the movies (*movies.csv*) dataset is formatted as:\n",
    "\n",
    "*movieId,title,genres*\n",
    "\n",
    "Where *genres* has the format:\n",
    "\n",
    "*Genre1|Genre2|Genre3...*\n",
    "\n",
    "The tags file (*tags.csv*) has the format:\n",
    "\n",
    "*userId,movieId,tag,timestamp*\n",
    "\n",
    "The *links.csv* file has the format:\n",
    "\n",
    "*movieId,imdbId,tmdbId*\n",
    "\n",
    "These files are uniformly formatted, it's easy to parse using split(). To train a recommender, we need to parse movies and ratings into two RDDs:\n",
    "- For each line in movies dataset, we create a tuple of (movieId, title).\n",
    "- For each line in ratings dataset, we create a tuple of (userId, movieId, rating)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'1', u'31', u'2.5'), (u'1', u'1029', u'3.0'), (u'1', u'1061', u'3.0')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the samll dataset\n",
    "small_ratings_path = os.path.join('data', 'ml-latest-small', 'ratings.csv')\n",
    "small_ratings_data = sc.textFile(small_ratings_path)\n",
    "small_ratings_header = small_ratings_data.take(1)[0]\n",
    "\n",
    "#Parse\n",
    "small_ratings = small_ratings_data.filter(lambda line: line!=small_ratings_header).map(lambda line: line.split(\",\"))\\\n",
    ".map(lambda tokens: (tokens[0], tokens[1], tokens[2])).cache()\n",
    "\n",
    "small_ratings.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'1', u'Toy Story (1995)'),\n",
       " (u'2', u'Jumanji (1995)'),\n",
       " (u'3', u'Grumpier Old Men (1995)')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_movies_path = os.path.join('data', 'ml-latest-small', 'movies.csv')\n",
    "small_movies_data = sc.textFile(small_movies_path)\n",
    "small_movies_header = small_movies_data.take(1)[0]\n",
    "\n",
    "small_movies = small_movies_data.filter(lambda line: line!=small_movies_header).map(lambda line: line.split(\",\"))\\\n",
    ".map(lambda tokens: (tokens[0], tokens[1])).cache()\n",
    "\n",
    "small_movies.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Collaborative filtering](https://en.wikipedia.org/wiki/Collaborative_filtering) is commonly used for recommender systems. These techniques aim to fill in the missing entries of a user-item association matrix. spark.mllib currently supports model-based collaborative filtering, in which users and products are described by a small set of latent factors that can be used to predict missing entries. spark.mllib uses the alternating least squares (ALS) algorithm to learn these latent factors. \n",
    "### Select ALS parameters using the dataset\n",
    "First, we need to split the dataset into train, validation and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_RDD, validation_RDD, test_RDD = small_ratings.randomSplit([6, 2, 2], seed = 0L)\n",
    "validation_for_predict_RDD = validation_RDD.map(lambda x: (x[0], x[1]))\n",
    "test_for_predict_RDD = test_RDD.map(lambda x: (x[0], x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For rank 4 the RMSE is 0.952401795348\n",
      "For rank 8 the RMSE is 0.959968407203\n",
      "For rank 12 the RMSE is 0.953274504895\n",
      "The best model was trained with rank 4\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.recommendation import ALS\n",
    "import math\n",
    "\n",
    "seed = 5L\n",
    "iterations = 10\n",
    "regularization_parameter = 0.1\n",
    "ranks = [4, 8, 12]\n",
    "errors = [0, 0, 0]\n",
    "err = 0\n",
    "tolerance = 0.02\n",
    "\n",
    "min_error = float('inf')\n",
    "best_rank = -1\n",
    "best_iteration = -1\n",
    "for rank in ranks:\n",
    "    model = ALS.train(train_RDD, rank, seed = seed, iterations = iterations,\n",
    "                     lambda_ = regularization_parameter)\n",
    "    predictions = model.predictAll(validation_for_predict_RDD).map(lambda r: ((r[0], r[1]), r[2])) #(userId,movieId),predicted_rating\n",
    "    rates_and_preds = validation_RDD.map(lambda r: ((int(r[0]), int(r[1])), float(r[2]))).join(predictions)#(userId, moveiId),(actual_rating, predicted_rating)\n",
    "    error = math.sqrt(rates_and_preds.map(lambda r: (r[1][0] - r[1][1])**2).mean())#square error between actual and predicted ratings.\n",
    "    errors[err] = error\n",
    "    err += 1\n",
    "    print 'For rank %s the RMSE is %s' % (rank, error)\n",
    "    if error < min_error:\n",
    "        min_error = error\n",
    "        best_rank = rank\n",
    "        \n",
    "print 'The best model was trained with rank %s' % best_rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The above code finds the best hyper-parameter(rank) for the model, now we need to try the model on test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For testing data, the RMSE is 0.943727800028\n"
     ]
    }
   ],
   "source": [
    "model = ALS.train(train_RDD, best_rank, seed=seed, iterations=iterations, lambda_=regularization_parameter)\n",
    "predictions = model.predictAll(test_for_predict_RDD).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "rates_and_preds = test_RDD.map(lambda r: ((int(r[0]), int(r[1])), float(r[2]))).join(predictions)\n",
    "error = math.sqrt(rates_and_preds.map(lambda r: (r[1][0] - r[1][1])**2).mean())\n",
    "\n",
    "print 'For testing data, the RMSE is %s' % (error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model with full dataset\n",
    "First let's load the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 24404096 entries in the full dataset.\n"
     ]
    }
   ],
   "source": [
    "# Load the full dataset\n",
    "full_ratings_path = os.path.join('data', 'ml-latest', 'ratings.csv')\n",
    "full_ratings_data = sc.textFile(full_ratings_path)\n",
    "full_ratings_header = full_ratings_data.take(1)[0]\n",
    "\n",
    "#Parse\n",
    "full_ratings = full_ratings_data.filter(lambda line: line!=full_ratings_header).map(lambda line: line.split(\",\"))\\\n",
    ".map(lambda tokens: (int(tokens[0]), int(tokens[1]), float(tokens[2]))).cache()\n",
    "\n",
    "print \"There are %s entries in the full dataset.\" % (full_ratings.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we train and test the model using the same method as we did for small dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RMSE on testing data is 0.831954381788\n"
     ]
    }
   ],
   "source": [
    "train_RDD, test_RDD = full_ratings.randomSplit([7, 3], seed = 0L)\n",
    "full_model = ALS.train(train_RDD, best_rank, seed=seed, iterations=iterations, lambda_ = regularization_parameter)\n",
    "\n",
    "test_for_predict_RDD = test_RDD.map(lambda r: (r[0], r[1]))\n",
    "\n",
    "predictions = full_model.predictAll(test_for_predict_RDD).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "rates_and_preds = test_RDD.map(lambda r: ((int(r[0]), int(r[1])), float(r[2]))).join(predictions)\n",
    "error = math.sqrt(rates_and_preds.map(lambda r: (r[1][0] - r[1][1])**2).mean())\n",
    "\n",
    "print 'The RMSE on testing data is %s' % (error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make recommendations\n",
    "Building a recommendation using collaborative filtering is not as predictiong new entries using previously generated model. Because we need to include the new user preference in order to compare them with existing users in the dataset. Therefore, whenever a new user ratings comes in, we need to train the system again. This is very expensive, the scalability is a problem, that's why we use Spark!\n",
    "\n",
    "Let's load the movies data first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, u'Toy Story (1995)'),\n",
       " (2, u'Jumanji (1995)'),\n",
       " (3, u'Grumpier Old Men (1995)')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the full dataset\n",
    "full_movies_path = os.path.join('data', 'ml-latest', 'movies.csv')\n",
    "full_movies_data = sc.textFile(full_movies_path)\n",
    "full_movies_header = full_movies_data.take(1)[0]\n",
    "\n",
    "#Parse\n",
    "full_movies = full_movies_data.filter(lambda line: line!=full_movies_header).map(lambda line: line.split(\",\"))\\\n",
    ".map(lambda tokens: (tokens[0], tokens[1], tokens[2])).cache()\n",
    "\n",
    "full_movies_titles = full_movies.map(lambda r: (int(r[0]), r[1]))\n",
    "full_movies_titles.take(3)\n",
    "\n",
    "#print \"There are %s movies in the complete dataset\" % (full_movies_titles.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(122880, 10), (147460, 1), (131080, 53)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_counts_and_avarages(id_rating_tuple):\n",
    "    nrating = len(id_rating_tuple[1])\n",
    "    return id_rating_tuple[0], (nrating, float(sum(x for x in id_rating_tuple[1])) / nrating)\n",
    "\n",
    "movieId_ratings_RDD = (full_ratings.map(lambda x: (x[1], x[2])).groupByKey())\n",
    "movieId_avg_ratings_RDD = movieId_ratings_RDD.map(get_counts_and_avarages)\n",
    "movie_rating_counts_RDD = movieId_avg_ratings_RDD.map(lambda x: (x[0], x[1][0]))\n",
    "movie_rating_counts_RDD.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add new user ratings\n",
    "\n",
    "Now let's fake some data for the new user, we need to rate some movies for the new user and put them in a new RDD. We give the user ID 0, because it's not assigned in the existing dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New user ratings: [(0, 1, 9), (0, 32, 7), (0, 296, 8), (0, 162376, 8), (0, 159858, 6), (0, 152081, 9), (0, 71899, 9), (0, 68793, 8), (0, 68137, 5), (0, 63436, 4), (0, 60397, 9), (0, 33794, 7), (0, 122886, 6), (0, 1721, 9), (0, 2011, 7)]\n"
     ]
    }
   ],
   "source": [
    "new_user_Id = 0\n",
    "\n",
    "# userId, movieId, rating\n",
    "new_user_ratings = [\n",
    "    (0, 1, 9), #Toy Story\n",
    "    (0, 32, 7), #Twelve Monkeys \n",
    "    (0, 296, 8), #Pulp Fiction\n",
    "    (0, 162376, 8), #Strang things\n",
    "    (0, 159858, 6), #The conjuring 2\n",
    "    (0, 152081, 9), #Zootopia\n",
    "    (0, 71899, 9), #Mary and Max\n",
    "    (0, 68793, 8), #Night at the museum\n",
    "    (0, 68137, 5), #Nana\n",
    "    (0, 63436, 4), #Saw V\n",
    "    (0, 60397, 9), #Mama Mia\n",
    "    (0, 33794, 7), #Batman Begins\n",
    "    (0, 122886, 6), #Star War: The force aweken \n",
    "    (0, 1721, 9), #Titanic\n",
    "    (0, 2011, 7), #Back to the future II\n",
    "]\n",
    "new_user_ratings_RDD = sc.parallelize(new_user_ratings)\n",
    "print \"New user ratings: %s\" % new_user_ratings_RDD.take(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we need to add this new user rating into our rating dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_data_with_new_rating_RDD = full_ratings.union(new_user_ratings_RDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And re-train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New model trained in 157.223 seconds.\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "t0 = time()\n",
    "new_model = ALS.train(full_data_with_new_rating_RDD, best_rank, seed=seed, \n",
    "                      iterations=iterations, lambda_ = regularization_parameter)\n",
    "tt = time() - t0\n",
    "print \"New model trained in %s seconds.\" % round(tt, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get recommendations\n",
    "\n",
    "Now, it's time to get some recommendations! We will get an RDD with all the movies the new user hasn't rated yet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Rating(user=0, product=116688, rating=2.7996465831532102),\n",
       " Rating(user=0, product=32196, rating=6.095657455529821),\n",
       " Rating(user=0, product=138744, rating=4.62930823409996)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_user_ratings_ids = map(lambda r: r[1], new_user_ratings)\n",
    "#Only keep the movies the user hasn't rated yet.\n",
    "new_user_unrated_movies_RDD = (full_movies.filter(lambda x: x[0] not in new_user_ratings_ids)\n",
    "                               .map(lambda x: (new_user_Id, x[0])))\n",
    "\n",
    "#Use the input RDD, new_user_unrated_movies_RDD, with new_model.predictAll() to predict new ratings for the movie\n",
    "new_user_recommendations_RDD = new_model.predictAll(new_user_unrated_movies_RDD)\n",
    "new_user_recommendations_RDD.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have all predictions, we can print out the top 25 movies and join them with the movies RDD to get the title, and ratings count to get movies with a minumum number of counts. First, let's take a look at the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(52224, ((2.2741304360240884, u'Turn of Faith (2002)'), 2)),\n",
       " (8194, ((6.377721071958565, u'Baby Doll (1956)'), 93)),\n",
       " (130730, ((6.641372912166924, u'Lemon Popsicle (1978)'), 3))]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_user_recommendations_rating_RDD = new_user_recommendations_RDD.map(lambda x: (x.product, x.rating))\n",
    "new_user_recommendations_rating_title_and_count_RDD = new_user_recommendations_rating_RDD.join \\\n",
    "(full_movies_titles).join(movie_rating_counts_RDD)\n",
    "\n",
    "new_user_recommendations_rating_title_and_count_RDD.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_user_recommendations_rating_title_and_count_RDD = \\\n",
    "    new_user_recommendations_rating_title_and_count_RDD.map(lambda r: (r[1][0][1], r[1][0][0], r[1][1]))\n",
    "#Now the data is flattened to [movieName, movieRating, num_ratings] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to get the highest rated recommendations for the new user, filtering out the movies with less than 25 ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top recommended movies (with more than 25 reviews): \n",
      "(u'Sense & Sensibility (2008)', 9.617256200119671, 31)\n",
      "(u'Boys (2014)', 9.581908124785942, 41)\n",
      "(u'North & South (2004)', 9.361230394901336, 318)\n",
      "(u'\"Very Potter Sequel', 9.329569886824654, 31)\n",
      "(u'Piper (2016)', 9.30138577062134, 88)\n",
      "(u'Bridegroom (2013)', 9.301033879213085, 26)\n",
      "(u'Pride and Prejudice (1995)', 9.241373327810845, 2510)\n",
      "(u'Long Way Round (2004)', 9.203669938324683, 29)\n",
      "(u'Anne of Green Gables: The Sequel (a.k.a. Anne of Avonlea) (1987)', 9.190376397097724, 293)\n",
      "(u'\"Shawshank Redemption', 9.141456020001094, 84455)\n",
      "(u'Anne of Green Gables (1985)', 9.127411772902109, 622)\n",
      "(u\"Schindler's List (1993)\", 9.073768668021366, 63889)\n",
      "(u'Wild China (2008)', 9.05449812547844, 91)\n",
      "(u'\"Very Potter Musical', 8.9969789880336, 75)\n",
      "(u'Dylan Moran Live: What It Is (2009)', 8.985010211520505, 59)\n",
      "(u\"Into the Forest of Fireflies' Light (2011)\", 8.944206628425253, 35)\n",
      "(u'Planet Earth (2006)', 8.940593396410696, 193)\n",
      "(u'Emma (2009)', 8.902786172496672, 314)\n",
      "(u'Life Is Beautiful (La Vita \\xe8 bella) (1997)', 8.900304878163984, 23539)\n",
      "(u'Iron Jawed Angels (2004)', 8.89943086170521, 32)\n",
      "(u'Prayers for Bobby (2009)', 8.891324326592704, 62)\n",
      "(u'Doctor Who: The Runaway Bride (2007)', 8.887787336563392, 216)\n",
      "(u'Feast (2014)', 8.860894364611232, 363)\n",
      "(u'Voices from the List (2004)', 8.847683906727854, 1826)\n",
      "(u'Paperman (2012)', 8.845605753462763, 1455)\n"
     ]
    }
   ],
   "source": [
    "top_movies = new_user_recommendations_rating_title_and_count_RDD. \\\n",
    "    filter(lambda r:r[2] >=25).takeOrdered(25, key = lambda x: -x[1])\n",
    "\n",
    "print ('Top recommended movies (with more than 25 reviews): \\n%s' % '\\n'.join(map(str, top_movies)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get individual ratings\n",
    "\n",
    "We can also use this to get the predicted rating for a particular movie for a given user. To do this, we just need to pass a single entry with the movie we want to predict to the predictAll method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Rating(user=0, product=116688, rating=2.7996465831532102)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_movie = sc.parallelize([(0, 300)]) #Quiz show (1994)\n",
    "individual_movie_rating_RDD = new_model.predictAll(new_user_unrated_movies_RDD)\n",
    "individual_movie_rating_RDD.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like this user may not like this movie.\n",
    "\n",
    "### Save the model\n",
    "\n",
    "We might wnat to persist this model for later use in our recommendations. Although a new model needs to be generated everytime we have new user ratings, it's still worth to save the current one. We can also save some of the RDDs we have generated, especially those that takes a long time to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.recommendation import MatrixFactorizationModel\n",
    "\n",
    "model_path = os.path.join('models', 'movie_lens_als')\n",
    "\n",
    "#model.save(sc, model_path)\n",
    "#new_model = MatrixFactorizationModel.load(sc, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
